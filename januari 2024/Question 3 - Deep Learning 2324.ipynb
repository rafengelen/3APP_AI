{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in\n",
    "\n",
    "- __FULL NAME__: Raf Engelen\n",
    "- __STUDENT NUMBER__:  r0901812\n",
    "- __CLASS__: 3APP1\n",
    "\n",
    "\n",
    "# Practical 3: Deep Learning - Flying Objects Classifier\n",
    "# Score: ... /10\n",
    "\n",
    "In the resources/images-folder you will find approximately 600 images of three fying objects (balloon, helicopter, and planes). Build a CNN in __Keras__ to classify these images in three classes.\n",
    "\n",
    "__Important:__ The design of the CNN is described below (__don't deviate from this design__). We'll be using the newer, __layered model__ approach (Data Preprocessing & Augmentation inside the model).\n",
    "\n",
    "Code the cells below, and when asked, answer the questions by typing the answer in the cell at the appropiate place (ANSWER:...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place all your necessary imports in this cell\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Create the CNN\n",
    "\n",
    "Create a sequential CNN with the following specifications (using tensorflow.keras, so as a list of layers):\n",
    "\n",
    "- Preprocessing layers:\n",
    "    - a resizing layer, to image size of 128x128\n",
    "    - a rescaling layer, to normalize the pixel values to a 0-1 range\n",
    "    - an augmentation layer, to add random horizontal flips\n",
    "    - an augmentation layer, to randomly translate the image by max 20% in each direction\n",
    "    - an augmentation layer, to randomly zoom in with a max factor of 20%\n",
    "- The CNN layers:\n",
    "    - a convolutional layer with 16 filters of size 7x7; relu is the activation function; the input size is 128x128, and we don't want the convolutions to change the size of the image\n",
    "    - a maxpooling layer with size 2x2\n",
    "    - a dropout layer; drop 15% of the existing connections\n",
    "    - a second convolutional layer with 64 filters of size 3x3; relu is the activation function; and we don't want the convolutions to change the size of the image\n",
    "    - a maxpooling layer with size 2x2\n",
    "    - a dropout layer; drop 15% of the existing connections\n",
    "    - a layer to convert the multi-dimensional output into a one-dimensional array\n",
    "    - a fully connected layer with 256 neurons and activation relu\n",
    "    - another fully connected layer with 128 neurons and activation relu\n",
    "    - a final layer with softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to create a sequential model with a list of layers\n",
    "\n",
    "model = Sequential()\n",
    "model = tf.keras.Sequential([\n",
    "  #a resizing layer, to image size of 128x128\n",
    "  layers.Resizing(128, 128),\n",
    "  #a rescaling layer, to normalize the pixel values to a 0-1 range\n",
    "  layers.Rescaling(1./255),\n",
    "  #an augmentation layer, to add random horizontal flips\n",
    "  layers.RandomFlip(\"horizontal\"),\n",
    "  #an augmentation layer, to randomly translate the image by max 20% in each direction\n",
    "  layers.RandomTranslation(0.2,0.2),\n",
    "  #an augmentation layer, to randomly zoom in with a max factor of 20%\n",
    "  layers.RandomZoom(0.2),\n",
    "\n",
    "  #CNN layers\n",
    "  #a convolutional layer with 16 filters of size 7x7; relu is the activation function; the input size is 128x128, and we don't want the convolutions to change the size of the image\n",
    "  layers.Conv2D(16, (7, 7), input_shape = (128, 128, 3), activation = 'relu'),\n",
    "  # a maxpooling layer with size 2x2\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  # a dropout layer; drop 15% of the existing connections\n",
    "  layers.Dropout(0.15),\n",
    "  #a second convolutional layer with 64 filters of size 3x3; relu is the activation function; and we don't want the convolutions to change the size of the image\n",
    "  layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "  # a maxpooling layer with size 2x2\n",
    "  layers.MaxPooling2D((2, 2)),\n",
    "  # a dropout layer; drop 15% of the existing connections\n",
    "  layers.Dropout(0.15),\n",
    "  #a layer to convert the multi-dimensional output into a one-dimensional array\n",
    "  layers.Flatten(),\n",
    "  #a fully connected layer with 256 neurons and activation relu\n",
    "  layers.Dense(256, activation=\"relu\"),\n",
    "  #another fully connected layer with 128 neurons and activation relu\n",
    "  layers.Dense(128, activation=\"relu\"),\n",
    "  #a final layer with softmax\n",
    "  layers.Dense(activation=\"sigmoid\", units=1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Compile the model\n",
    "\n",
    "Compile the model: use Adam as your optimizer, with a learning rate of 0.001, and a **sparse** categorical crossentropy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to compile the model\n",
    "model.compile(\n",
    "    optimizer = optimizers.Adam(learning_rate=0.001), \n",
    "    loss = 'sparse_categorical_crossentropy', \n",
    "    metrics = ['accuracy']\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Create a training, validation, and test set\n",
    "\n",
    "Use the **image_dataset_from_directory** method to create a training, validation, and test dataset. **Do not** use an ImageDataGenerator. \n",
    "\n",
    "Make sure you create the training and validation set from the 'resources/images/train/' folder, using an 80-20% split, with a seed of 5. The test set, is just from the 'resources/images/test/' folder. \n",
    "All datasets, use a batch size of 16, and the labels should be taken from the parent folder, using a **sparse encoding**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 508 files belonging to 3 classes.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When passing `label_mode=\"binary\"`, there must be exactly 2 class_names. Received: class_names=['balloon', 'helicopter', 'planes']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m validation_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create the training dataset from the 'train' directory\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m \u001b[43mimage_dataset_from_directory\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirectory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./resources/images/train\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minferred\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimage_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m123\u001b[39;49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Create the validation dataset from the 'train' directory\u001b[39;00m\n\u001b[0;32m     21\u001b[0m validation_ds \u001b[38;5;241m=\u001b[39m image_dataset_from_directory(\n\u001b[0;32m     22\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./resources/images/train\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     23\u001b[0m     labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferred\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m\n\u001b[0;32m     30\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\rafen\\Desktop\\3APP_AI\\.venv\\lib\\site-packages\\keras\\src\\utils\\image_dataset.py:221\u001b[0m, in \u001b[0;36mimage_dataset_from_directory\u001b[1;34m(directory, labels, label_mode, class_names, color_mode, batch_size, image_size, shuffle, seed, validation_split, subset, interpolation, follow_links, crop_to_aspect_ratio, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m image_paths, labels, class_names \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mindex_directory(\n\u001b[0;32m    211\u001b[0m     directory,\n\u001b[0;32m    212\u001b[0m     labels,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m     follow_links\u001b[38;5;241m=\u001b[39mfollow_links,\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m--> 221\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWhen passing `label_mode=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`, there must be exactly 2 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_names. Received: class_names=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    224\u001b[0m     )\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m subset \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboth\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    227\u001b[0m     (\n\u001b[0;32m    228\u001b[0m         image_paths_train,\n\u001b[0;32m    229\u001b[0m         labels_train,\n\u001b[0;32m    230\u001b[0m     ) \u001b[38;5;241m=\u001b[39m dataset_utils\u001b[38;5;241m.\u001b[39mget_training_or_validation_split(\n\u001b[0;32m    231\u001b[0m         image_paths, labels, validation_split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: When passing `label_mode=\"binary\"`, there must be exactly 2 class_names. Received: class_names=['balloon', 'helicopter', 'planes']"
     ]
    }
   ],
   "source": [
    "\n",
    "# Use this cell to create your training, validation, and test set\n",
    "\n",
    "# Set the parameters for your data\n",
    "batch_size = 16\n",
    "image_size = (64, 64)\n",
    "validation_split = 0.2\n",
    "\n",
    "# Create the training dataset from the 'train' directory\n",
    "train_ds = image_dataset_from_directory(\n",
    "    directory='./resources/images/train',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    subset='training',\n",
    "    seed=5\n",
    ")\n",
    "\n",
    "# Create the validation dataset from the 'train' directory\n",
    "validation_ds = image_dataset_from_directory(\n",
    "    directory='./resources/images/train',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation',\n",
    "    seed=5\n",
    ")\n",
    "\n",
    "# Create the testing dataset from the 'test' directory\n",
    "test_ds = image_dataset_from_directory(\n",
    "    directory='./resources/images/test',\n",
    "    labels='inferred',\n",
    "    label_mode='binary',\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Train the model\n",
    "\n",
    "Train your model for 10 epochs (otherwise it will take too long). Don't forget to store your training and validation losses in a history object, so you can visualize your training later.\n",
    "\n",
    "If you get 'WARNING:tensorflow: Using a while_loop for converting', you can simply ignore them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this cell to train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What accuracy do you achieve on the training set?\n",
    "\n",
    "ANSWER: \n",
    "\n",
    "#### Question: What accuracy do you achieve on the validation set?\n",
    "\n",
    "ANSWER: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Visualizing the training process\n",
    "\n",
    "Plot the training and validation loss of your trainingcycles, using the history object that you created during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use this cell to add the plotlosses function, and plot the history of your training\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: is your model over-, under-, or well fitted, looking at the plotted losses above (and explain why)?\n",
    "\n",
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F. Evaluation of the model\n",
    "\n",
    "Add code in the cell below to evaluate your model, using the test set you created earlier. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question: What is your accuracy on the test set?\n",
    "\n",
    "ANSWER:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G. Predict & Plot\n",
    "\n",
    "In the cell below, add code to predict & plot 10 test images from the test set. Below each image, you should use as a label the actual label (a), and the predicted label (p) of your model. If they are the same, the textcolor of the label should be green, red otherwise.\n",
    "\n",
    "\n",
    "Hints: \n",
    "- Getting 5 images/label pairs from a dataset, you can use: images, labels = next(iter(dataset.take(5)))\n",
    "- For retrieving different labels from a dataset, you can use: dataset.class_names\n",
    "- Ploting an image from a dataset, you have to typecast the pixel values to 'uint8', like: image.numpy().astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict and plot 10 test images\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac19fea7ca157f65070a947d4f1cdd50788aaffbd4bca54f0297c51647d6a9e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
