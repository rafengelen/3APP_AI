{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi-v3\n",
    "\n",
    "Gym provides different game environments which we can plug into our code and train an agent with. The library takes care of the API for providing all the information that our agent requires, like possible actions, score, and current state. We just need to focus on the algorithm part of our agent.\n",
    "\n",
    "We'll be using the Gym environment called Taxi-v3, which all of the details explained in previous Notebook were pulled from. The objectives, rewards, and actions are all the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Gym(nasium)\n",
    "\n",
    "We need to install Gymnasium first (Gym from OpenAI is no longer supported). Docs: https://gymnasium.farama.org/  \n",
    "\n",
    "Executing the following should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasiumNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading gymnasium-0.29.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\rafen\\desktop\\3app_ai\\.venv\\lib\\site-packages (from gymnasium) (1.24.4)\n",
      "Collecting cloudpickle>=1.2.0 (from gymnasium)\n",
      "  Downloading cloudpickle-3.0.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\rafen\\desktop\\3app_ai\\.venv\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in c:\\users\\rafen\\desktop\\3app_ai\\.venv\\lib\\site-packages (from gymnasium) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\rafen\\desktop\\3app_ai\\.venv\\lib\\site-packages (from importlib-metadata>=4.8.0->gymnasium) (3.17.0)\n",
      "Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
      "   ---------------------------------------- 0.0/953.9 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 235.5/953.9 kB 3.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 512.0/953.9 kB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 686.1/953.9 kB 4.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 953.9/953.9 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: farama-notifications, cloudpickle, gymnasium\n",
      "Successfully installed cloudpickle-3.0.0 farama-notifications-0.0.4 gymnasium-0.29.1\n"
     ]
    }
   ],
   "source": [
    "pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install \"gymnasium[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install gymnasium[toy-text]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gym's interface\n",
    "\n",
    "Once installed, we can load the game environment and render what it looks like. When we're all done, we can close the rendering an environment with env.close(). This will clear all memory. So only use when you want to start from scratch again.\n",
    "We can also print the action and state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's just render the environment for 10 seconds, and close the window again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment\n",
    "state, info = env.reset() # reset to make sure with start with a randomly chosen start-state\n",
    "\n",
    "print(\"Action Space: {}\".format(env.action_space)) # 6 different actions we can take - discrete actions, no continuous actions (no 'range') \n",
    "print(\"State Space: {}\\n\".format(env.observation_space)) # 500 different states\n",
    "\n",
    "print(\"Current state: %d\" % env.unwrapped.s)\n",
    "\n",
    "env.render() # visualize the environment - we only need to call render once\n",
    "time.sleep(10)\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder of our problem:\n",
    "\n",
    "- The filled square represents the taxi, which is yellow without a passenger and green with a passenger.\n",
    "- The pipe (\"|\") represents a wall which the taxi cannot cross.\n",
    "- R, G, Y, B are the possible pickup and dropoff locations. The blue letter represents the current passenger pick-up location, and the purple letter is the current destination. In the illustration, these colors are reversed.\n",
    "\n",
    "- As verified by the prints, we have an Action Space of size 6 (south, north, east, west, pickup and dropoff) and a State Space of size 500 (the taxi's location x the passenger's location x the destination location).\n",
    "- The current state is randomly chosen (a state between 0 and 499)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Back to our illustration\n",
    "\n",
    "We can actually take our illustration, encode its state, and give it to the environment to render in Gym.\n",
    "\n",
    "<img src=\"./resources/taxi.png\" style=\"height: 350px\"/>\n",
    "\n",
    "Recall that we have the taxi at row 3, column 1, our passenger is at location 2 (=Y) (out of the 4 possible locations), and our destination is location 0 (=R). Using the Taxi-v3 state encoding method, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\")\n",
    "env.reset()\n",
    "\n",
    "# now, let's manually encode the state\n",
    "state = env.unwrapped.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger location, destination location)\n",
    "print(\"State:\", state)\n",
    "\n",
    "env.unwrapped.s = state\n",
    "\n",
    "env.render() # visualize the environment - we only need to call render once\n",
    "time.sleep(10)\n",
    "env.close()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initial State - Exercise\n",
    "\n",
    "Generate the state where the taxi is in the lower right corner. The passenger is in the taxi and the destination is B. What is the color of the taxi right now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step-method\n",
    "\n",
    "The agent performs an action by using the step-method:\n",
    "\n",
    "```python\n",
    "state, reward, done, truncated, info = env.step(action)\n",
    "```\n",
    "\n",
    "Remember: There are 6 actions (0=south, 1=north, 2=east, 3=west, 4=pickup and 5=dropoff).\n",
    "\n",
    "The step-method returns:\n",
    "\n",
    "- state: the new state\n",
    "- reward: if your action was beneficial or not\n",
    "- done: indicates if we have successfully picked up and dropped off a passenger, also called one episode\n",
    "- truncated: is timelimit reached, or agent out of bounds?\n",
    "- info: additional info such as performance and latency for debugging purposes\n",
    "\n",
    "Let's go back to the illustrations state and try the different actions.\n",
    "\n",
    "<img src=\"./resources/taxi.png\" style=\"height: 250px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure we start in the position like the image, and let's take a step west (to the left). So, we'll hit a wall, and we should receive a penalty for not reaching the destination, and we should remain in the same state (because we hit the wall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment again\n",
    "state, info = env.reset() # reset to make sure with a clean env\n",
    "\n",
    "state = env.unwrapped.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"Initial state:\", state)\n",
    "env.unwrapped.s = state\n",
    "\n",
    "env.render() # visualize the environment - we only need to call render once\n",
    "time.sleep(10)\n",
    "state, reward, done, truncated, info = env.step(3) # go west\n",
    "time.sleep(10)\n",
    "env.close()\n",
    "\n",
    "print(\"New state: %d, reward: %d\" % (state, reward)) # a -1 penalty for every wall hit and the taxi won't move anywhere"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's do it again, but go north instead of west. So we won't hit a wall, but still, we haven't reached our destination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment again\n",
    "state, info = env.reset() # reset to make sure with a clean env\n",
    "\n",
    "state = env.unwrapped.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger index, destination index)\n",
    "print(\"Initial state:\", state)\n",
    "env.unwrapped.s = state\n",
    "\n",
    "env.render() # visualize the environment - we only need to call render once\n",
    "time.sleep(10)\n",
    "state, reward, done, truncated, info = env.step(1) # go north\n",
    "time.sleep(10)\n",
    "done = True\n",
    "\n",
    "print(\"New state: %d, reward: %d\" % (state, reward)) #\n",
    "# taxi moves up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Step-method - Exercise\n",
    "\n",
    "Generate the state again where the taxi is in the lower right corner. The passenger is in the taxi and the destination is B.\n",
    "\n",
    "Next, carry out the two steps: first drive the taxi to the destination location, and next drop off the passenger. \n",
    "Render the environment at each step, print the new state number, the reward and done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The Reward Table\n",
    "\n",
    "When the Taxi environment is created, there is an initial Reward table that's also created, called *P*. **We can think of it like a matrix that has the number of states as rows and the number of actions as columns, i.e. a states × actions matrix**.\n",
    "\n",
    "Since every state is in this matrix, we can see the default reward values assigned for state 479."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.unwrapped.s = 479\n",
    "env.unwrapped.P[479]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has the structure\n",
    "\n",
    "``\n",
    "{action: [(probability, nextstate, reward, done)]}.\n",
    "``\n",
    "\n",
    "A few things to note:\n",
    "\n",
    "- The 0-5 corresponds to the actions (south, north, east, west, pickup, dropoff) the taxi can perform at our current state.\n",
    "- In this environment, the probability of each action is always 1.0.\n",
    "- The nextstate is the state we would be in if we take the action at this index of the dictionary.\n",
    "- All the movement actions have a -1 reward, the pickup action has a -10 reward, the dropoff actions has +20 reward in this particular state.\n",
    "- done is used to tell us when we have successfully dropped off a passenger at the right location (action 5 in this state). Each successful dropoff is the end of an episode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Solving the environment without Reinforcement Learning\n",
    "\n",
    "Let's see what will happen if we try to brute-force our way to solving the problem without RL.\n",
    "\n",
    "We'll create an infinite loop which runs until one passenger reaches one destination (one episode), or in other words, when the received reward is 20. The `env.action_space.sample()` method automatically selects one random action from the set of all possible actions.\n",
    "\n",
    "Let's see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment again\n",
    "state, info = env.reset() # reset to make sure with a clean env\n",
    "\n",
    "env.unwrapped.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "finished = False\n",
    "\n",
    "while not finished:\n",
    "    action = env.action_space.sample() \n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "\n",
    "    epochs += 1\n",
    "    finished = done or truncated\n",
    "env.close()\n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not good. Our agent takes thousands (?) of timesteps and makes lots of wrong drop offs to deliver just one passenger to the right destination.\n",
    "\n",
    "This is because we aren't learning from past experience. We can run this over and over, and it will never optimize. **The agent has no memory of which action was best for each state**, which is exactly what Reinforcement Learning will do for us."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Q-learning\n",
    "\n",
    "We are going to use a *simple* RL algorithm called Q-learning which will give our agent some memory. Essentially, Q-learning lets the agent use the environment's rewards to learn, over time, the best action to take in a given state.\n",
    "\n",
    "Therefore the Q-learning algorithm uses a Q-Table. The Q-table is a matrix where we have a row for every state (500) and a column for every action (6). The matrix is first initialized to 0, and then values are updated during training.\n",
    "\n",
    "<img src=\"./resources/qtable.png\" style=\"height: 650px\"/>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal action at every state is the action with the highest Q-value. So for state 328 the highest value is -1.971 (=North). For state 499 the highest value is 29 (=West). These actions indeed seem to be the best options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"human\") # load the game environment again\n",
    "state, info = env.reset() # reset to make sure with a clean env\n",
    "\n",
    "env.unwrapped.s = 328\n",
    "env.render()\n",
    "print(\"North is best?\\n\")\n",
    "time.sleep(5)\n",
    "\n",
    "env.unwrapped.s = 499\n",
    "env.render()\n",
    "print(\"West is best?\")\n",
    "time.sleep(5)\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training the Agent\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create the training algorithm that will update this Q-table as the agent explores the environment over 100 000 of episodes (of course you don't need to write this algorithm yourself).\n",
    "\n",
    "BTW: When we create the environment, we will not be rendering it, because this slows us down way too much. Especially in the next section, where we'll be training a q-table. After this training, we'll use the 'human' rendering again.\n",
    "\n",
    "\n",
    "Next, we'll initialize the Q-table to a 500×6 matrix of zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\")\n",
    "state, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "for i in range(1, 100001):\n",
    "    state, info = env.reset()\n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample() # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state]) # Exploit learned values\n",
    "        next_state, reward, done, truncated, info = env.step(action) \n",
    "        finished = done or truncated\n",
    "\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "\n",
    "print(\"Training finished.\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the Q-table has been established over 100 000 episodes, let's see what the Q-values are for our two states. Are North and West indeed the two most preferable moves for the two states?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_table[328])\n",
    "print(q_table[499])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Evaluating the agent\n",
    "\n",
    "Let's evaluate the performance of our agent. We don't need to explore actions any further, so now the next action is always selected using the best Q-value. Training is done, so no more balancing between exploration versus exploitation. Only exploit the knowledge gathered so far, in the Q-table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "total_epochs, total_penalties = 0, 0\n",
    "episodes = 100\n",
    "\n",
    "for _ in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        action = np.argmax(q_table[state]) # take the action with the highest q-value\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        finished = done or truncated\n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "\n",
    "        epochs += 1\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {episodes} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the evaluation that the agent's performance improved significantly and it incurred no penalties, which means it performed the correct pickup/dropoff actions with 100 different passengers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Solving the environment with Reinforcement Learning - Exercise\n",
    "\n",
    "Now that our agent is trained, we can use our human rendering again, and use our trained q-table to solve the taxi problem with 328 as the initial state (by analogy with section **7. Solving the environment without RL** of this notebook). Print the timesteps taken and penalties incurred. Pretty impressive! No?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Solving the environment with RL - Exercise\n",
    "\n",
    "Solve the problem for these two initial states.\n",
    "\n",
    "<img src=\"./resources/taxi1.png\" style=\"height: 300px\"/>\n",
    "<img src=\"./resources/taxi2.png\" style=\"height: 300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve the problem with the first/second initial state\n",
    "# so encode the different states, like state = env.encode(row,column,passenger,destiny), and do the same thing: exploit the knowledge from the q-table: go for np.argmax(q_table[state])!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "3762fa38335283772957b8ad4850e1d344741e862c3aa7ec1c26d17780db6975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
