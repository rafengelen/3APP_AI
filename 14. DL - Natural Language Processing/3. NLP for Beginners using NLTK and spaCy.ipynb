{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP for Beginners using NLTK and spaCy\n",
    "\n",
    "NLTK is a powerful Python package that provides a set of diverse natural languages algorithms. It is free, Open Source, easy to use, well documented and it has a large community. NLTK consists of the most common algorithms such as tokenizing, part-of-speech tagging, stemming, sentiment analysis, topic segmentation, and named entity recognition. NLTK helps the computer to analyse, preprocess and understand written text.\n",
    "\n",
    "You can install it by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting click (from nltk)\n",
      "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2023_2024\\dl_env\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Obtaining dependency information for regex>=2021.8.3 from https://files.pythonhosted.org/packages/33/03/91c9509b43154795fb848a4cf8cef5b37302b3b3ccf8a9763046ea528c6b/regex-2023.10.3-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB 1.0 MB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata\n",
      "  Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2023_2024\\dl_env\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading regex-2023.10.3-cp310-cp310-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.6/269.6 kB 8.1 MB/s eta 0:00:00\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 97.9/97.9 kB 5.5 MB/s eta 0:00:00\n",
      "Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2023.10.3 tqdm-4.66.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\u0040810\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\u0040810\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\u0040810\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizing\n",
    "\n",
    "When we deal with text, we need to break it down into smaller pieces for analysis. This is\n",
    "where tokenization comes into the picture. It is the process of dividing the input text into a\n",
    "set of pieces like words or sentences. These pieces are called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence tokenizer:\n",
      "['Do you know how tokenization works?', \"It's actually quite interesting!\", \"Let's analyze a couple of sentences and figure it out.\"]\n",
      "\n",
      "Word tokenizer:\n",
      "['Do', 'you', 'know', 'how', 'tokenization', 'works', '?', 'It', \"'s\", 'actually', 'quite', 'interesting', '!', 'Let', \"'s\", 'analyze', 'a', 'couple', 'of', 'sentences', 'and', 'figure', 'it', 'out', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# define input text\n",
    "input_text = \"Do you know how tokenization works? It's actually quite interesting! Let's analyze a couple of sentences and figure it out.\"\n",
    "\n",
    "# sentence tokenizer\n",
    "print(\"\\nSentence tokenizer:\")\n",
    "print(sent_tokenize(input_text))\n",
    "\n",
    "# word tokenizer\n",
    "print(\"\\nWord tokenizer:\")\n",
    "print(word_tokenize(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwords\n",
    "\n",
    "Stopwords are considered as noise in the text. Text may contain stopwords such as *is, am, are, this, a, an, the,* etc.\n",
    "\n",
    "It is clear that you first need a list of stopwords so these words can be removed. This list can be easily created as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'had', 'than', 'each', 'few', 'once', 'wouldn', 'but', \"needn't\", 'with', 'then', 'itself', 'in', 'his', 'for', \"you'll\", \"won't\", 'now', 'yours', 'such', 'where', 'its', 'our', 'because', 'some', 'we', 'hasn', 'didn', 'through', \"it's\", 'hadn', \"hasn't\", 'theirs', \"don't\", 'your', 'why', 'above', 'or', 'been', 'very', \"aren't\", \"mustn't\", 'during', \"couldn't\", 'doesn', 'weren', 'does', \"doesn't\", \"wasn't\", 'there', 'and', \"she's\", \"weren't\", 'd', 'own', 'can', 'between', \"didn't\", 'if', 'after', 'should', 'do', \"wouldn't\", 'isn', 'mightn', 'those', 'wasn', 'this', 'nor', 'a', \"isn't\", 'these', 'all', 'more', 'has', 's', 'only', 'under', 'mustn', 'no', 'who', 'how', \"haven't\", 'again', 'is', 've', 'aren', 'when', \"hadn't\", 'myself', 'down', 'into', 'he', 'an', 'to', 'have', 'ourselves', 'too', 'm', \"shan't\", 'couldn', 'by', \"you'd\", 'what', 'him', 'i', 'over', 'out', 'of', 'ain', 'until', 'whom', 'yourself', 'not', \"you've\", 'was', 'are', 'on', 'same', \"mightn't\", 'before', 'below', 'me', 'at', 'them', 'yourselves', 'themselves', 'most', 'ma', 'as', 'which', 'any', 'against', 'up', 'about', \"shouldn't\", 'she', 'himself', 'were', 'here', 'haven', 'off', 'will', 'be', 'ours', 'did', 'the', 'my', 'don', 'am', 't', 're', 'needn', 'll', 'being', 'y', 'further', 'both', 'shouldn', \"should've\", 'having', 'o', 'it', \"you're\", 'herself', 'they', 'doing', 'shan', 'just', 'from', 'hers', 'won', 'you', \"that'll\", 'while', 'so', 'that', 'other', 'her', 'their'}\n"
     ]
    }
   ],
   "source": [
    "# create stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print all Dutch stopwords -  Exercise\n",
    "\n",
    "Write a little program that prints all Dutch stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print stopwords in dutch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords and punctuation - Exercise\n",
    "\n",
    "Now write a function `words()` that has a string as input parameter and returns all the words in that string without the stopwords. Also get rid of punctuation. The output of the input_text above should be:\n",
    "\n",
    "```\n",
    "Words without stopwords:  ['know', 'tokenization', 'works', 'actually', 'quite', 'interesting', 'let', 'analyze', 'couple', 'sentences', 'figure']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without stopwords:  ['know', 'tokenization', 'works', 'actually', 'quite', 'interesting', 'let', 'analyze', 'couple', 'sentences', 'figure']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "def words (input_text):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    output = []\n",
    "    for word in tokenizer.tokenize(input_text):\n",
    "        if word.lower() not in stop_words:\n",
    "            output.append(word.lower())\n",
    "    return output\n",
    "\n",
    "print(\"Words without stopwords: \", words(input_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stemming\n",
    "\n",
    "When working with text, we have to deal with different forms of the same word. For example, the word *sing* can appear in many forms such as *sang, singer, singing, singer,* and so on. When we analyze text, it's useful to reduce words in their different forms into a base form. This will enable us to extract useful statistics to analyze the input text.\n",
    "\n",
    "Stemming is one way to achieve this. It is basically a process that cuts off the ends of words to extract their base forms. Let's see how to do it using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "       INPUT WORD          PORTER        SNOWBALL       LANCASTER \n",
      " ====================================================================\n",
      "         writing           write           write            writ\n",
      "     connections         connect         connect         connect\n",
      "       connected         connect         connect         connect\n",
      "      connecting         connect         connect         connect\n",
      "           horse            hors            hors            hors\n",
      "       randomize          random          random          random\n",
      "        possibly         possibl         possibl            poss\n",
      "       provision          provis          provis          provid\n",
      "        hospital          hospit          hospit          hospit\n",
      "            kept            kept            kept            kept\n",
      "        scratchy        scratchi        scratchi        scratchy\n",
      "          calves            calv            calv            calv\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "input_words = ['writing', 'connections', 'connected', 'connecting', 'horse', 'randomize', 'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'calves']\n",
    "\n",
    "# create various stemmer objects\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "\n",
    "# create a list of stemmer names for display\n",
    "stemmer_names = ['PORTER', 'SNOWBALL', 'LANCASTER']\n",
    "formatted_text = '{:>16}' * (len(stemmer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *stemmer_names), '\\n', '='*68)\n",
    "\n",
    "# stem each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, porter.stem(word), snowball.stem(word), lancaster.stem(word)]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the three stemmers above is the level of strictness that's used to arrive at the base form. The Porter stemmer is the least in terms of strictness (\"possibly\" becomes \"possibl\") and Lancaster is the strictest (\"possibly\" becomes \"poss\").\n",
    "\n",
    "Note that the result might not be an actual word. All the three stemmers said that the base form of \"calves\" is \"calv\", which is not a real word.\n",
    "\n",
    "On the other hand all the three stemmers reduced \"connections, connected, connecting\" to a correct common word \"connect\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization - Exercise\n",
    "\n",
    "Lemmatization is another way of reducing words to their base form. The lemmatization process uses a vocabulary and morphological analysis of words. It obtains the base forms by removing word endings such as ing or ed. This\n",
    "base form of a word is known as a lemma. If you lemmatize the word \"calves\", you\n",
    "should get \"calf\" as the output. One thing to note is that the output depends on whether the word is a verb or a noun.\n",
    "\n",
    "Before using lemmatization, we have to download WordNet, a large lexical database of English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now write a little program to lemmatize the same `input_words` as above. Use the `lemmatize`-method from the `WordNetLemmatizer`-class. This method has two parameters: the first parameter is the word to be lemmatized, the second parameter is the type of output (pos='n' for a noun lemma, pos='v' for a verb lemma). The output should be something like this:\n",
    "\n",
    "```\n",
    "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
    " ===========================================================================\n",
    "                 writing                 writing                   write\n",
    "             connections              connection             connections\n",
    "               connected               connected                 connect\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "               INPUT WORD         NOUN LEMMATIZER         VERB LEMMATIZER \n",
      " ===========================================================================\n",
      "                 writing                 writing                   write\n",
      "             connections              connection             connections\n",
      "               connected               connected                 connect\n",
      "              connecting              connecting                 connect\n",
      "                   horse                   horse                   horse\n",
      "               randomize               randomize               randomize\n",
      "                possibly                possibly                possibly\n",
      "               provision               provision               provision\n",
      "                hospital                hospital                hospital\n",
      "                    kept                    kept                    keep\n",
      "                scratchy                scratchy                scratchy\n",
      "                  calves                    calf                   calve\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_words = ['writing', 'connections', 'connected', 'connecting', 'horse', 'randomize', 'possibly', 'provision', 'hospital', 'kept', 'scratchy', 'calves']\n",
    "\n",
    "# Create lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Create a list of lemmatizer names for display\n",
    "lemmatizer_names = ['NOUN LEMMATIZER', 'VERB LEMMATIZER']\n",
    "formatted_text = '{:>24}' * (len(lemmatizer_names) + 1)\n",
    "print('\\n', formatted_text.format('INPUT WORD', *lemmatizer_names), '\\n', '='*75)\n",
    "\n",
    "# Lemmatize each word and display the output\n",
    "for word in input_words:\n",
    "    output = [word, lemmatizer.lemmatize(word, pos='n'), lemmatizer.lemmatize(word, pos='v')]\n",
    "    print(formatted_text.format(*output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the noun lemmatizer works differently than the verb lemmatizer when it\n",
    "comes to words like writing or calves. If you compare these outputs to stemmer outputs, you\n",
    "will see that there are differences too. The lemmatizer outputs are all meaningful whereas\n",
    "stemmer outputs may or may not be meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POS Tagging\n",
    "\n",
    "The target of Part-of-Speech (POS) Tagging is to identify the grammatical group of a given word, whether it is a noun, pronoun, adjective, verb, adverb, etc. based on the context. POS Tagging looks for relationships within the sentence and assigns a corresponding tag to the word.\n",
    "\n",
    "We will use spaCy, a different Python library for NLP because it gives better results than NLTK for POS Tagging and Named Entity Recognition. First install spaCy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: spacy in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (1.23.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (63.2.0)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll download a trained language model (we need a 'model', a 'brain', to do POS and NER). We'll download the English and Dutch version of a small language model that's pretrained on written web text like blogs, news, comments,...).\n",
    "Restart the kernel afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 10.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nl_core_news_sm in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (3.4.0)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from nl_core_news_sm) (3.4.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (0.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (63.2.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (2.0.7)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (1.23.4)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (0.6.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (2.0.8)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (2.4.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (3.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (1.9.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl_core_news_sm) (3.0.10)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (2022.9.24)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (1.26.12)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->nl_core_news_sm) (2.1.1)\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 09:42:47.658490: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-10-20 09:42:47.658779: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-20 09:42:50.938143: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-10-20 09:42:50.938344: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-20 09:42:50.944810: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: G_NB110860\n",
      "2022-10-20 09:42:50.945097: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: G_NB110860\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!py -m spacy download en_core_web_sm nl_core_news_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll try to load in the language model (if this fails, check below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If loading the language model fails, it's probably due to linking errors (your python env isn't aware that you've download a spacy env).\n",
    "So, let's use a shortcut, that fixes this (at least it did for me)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the full\n",
      "pipeline package name 'en_core_web_sm' instead.\n",
      "Collecting en-core-web-sm==3.4.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.0/en_core_web_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 11.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from en-core-web-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.23.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (63.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.0) (2.1.1)\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 09:42:58.193515: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-10-20 09:42:58.193693: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-20 09:43:01.425155: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-10-20 09:43:01.425774: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-20 09:43:01.431517: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: G_NB110860\n",
      "2022-10-20 09:43:01.431799: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: G_NB110860\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!py -m spacy download en "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the core spaCy English model, we'll create a small spaCy document/text, that we will be using to perform Part-of-Speech tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sen = sp(\"I like to play football. I hated it in my childhood though.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spaCy document object has several attributes that can be used to perform a variety of tasks. For instance, to print the text of the document, the text attribute is used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I like to play football. I hated it in my childhood though.\n"
     ]
    }
   ],
   "source": [
    "print(sen.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the pos_ attribute returns the POS tag. And finally, to get the explanation of the POS tag, we can use the spacy.explain() method and pass it the tag name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hated\n",
      "VERB\n",
      "verb, past tense\n"
     ]
    }
   ],
   "source": [
    "print(sen[7])\n",
    "print(sen[7].pos_)\n",
    "print(spacy.explain(sen[7].tag_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print all the POS tags (we've improved the readability by adding 12 spaces between the text and the POS tag and then another 10 spaces between the POS tags and the explanation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON       pronoun, personal\n",
      "like         VERB       verb, non-3rd person singular present\n",
      "to           PART       infinitival \"to\"\n",
      "play         VERB       verb, base form\n",
      "football     NOUN       noun, singular or mass\n",
      ".            PUNCT      punctuation mark, sentence closer\n",
      "I            PRON       pronoun, personal\n",
      "hated        VERB       verb, past tense\n",
      "it           PRON       pronoun, personal\n",
      "in           ADP        conjunction, subordinating or preposition\n",
      "my           PRON       pronoun, possessive\n",
      "childhood    NOUN       noun, singular or mass\n",
      "though       ADV        adverb\n",
      ".            PUNCT      punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "for word in sen:\n",
    "    print(f'{word.text:{12}} {word.pos_:{10}} {spacy.explain(word.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another cool thing about spaCy is, that you can use the dependency visualizer to show Part-of-Speech tags and syntactic dependencies. Maybe you can try some other sentences to visualise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"6c9b37cd88c94e8284aa933fc57a8e31-0\" class=\"displacy\" width=\"2150\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">like</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">play</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">football.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">I</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">hated</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">it</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">my</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">childhood</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">though.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-1\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-2\" stroke-width=\"2px\" d=\"M245,264.5 C245,89.5 570.0,89.5 570.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570.0,266.5 L578.0,254.5 562.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1265.0,266.5 L1273.0,254.5 1257.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-6\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,89.5 1445.0,89.5 1445.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1445.0,266.5 L1453.0,254.5 1437.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-7\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,177.0 1790.0,177.0 1790.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-8\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,89.5 1795.0,89.5 1795.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1795.0,266.5 L1803.0,254.5 1787.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-9\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,2.0 1975.0,2.0 1975.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-6c9b37cd88c94e8284aa933fc57a8e31-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1975.0,266.5 L1983.0,254.5 1967.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "sen = sp(\"I like to play football. I hated it in my childhood though.\")\n",
    "displacy.render(sen, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging in Dutch - Exercise\n",
    "\n",
    "POS Tagging can be done in Dutch as well. You will probably have to install the Dutch model.\n",
    "\n",
    "Use this sentences as input: \"De concentratie broeikasgassen die bijdragen aan de verandering van het klimaat, heeft opnieuw een recordhoogte bereikt.\" The output should be as follows:\n",
    "\n",
    "```\n",
    "De           DET        Art|bep|zijdofmv|neut__Definite=Def|PronType=Art\n",
    "concentratie NOUN       N|soort|ev|neut__Number=Sing\n",
    "broeikasgassen ADP        Prep|voor__AdpType=Prep\n",
    "die          PRON       Pron|aanw|neut|attr__PronType=Dem\n",
    "bijdragen    NOUN       N|soort|mv|neut__Number=Plur\n",
    "aan          ADP        Prep|voor__AdpType=Prep\n",
    "de           DET        Art|bep|zijdofmv|neut__Definite=Def|PronType=Art\n",
    "verandering  NOUN       N|soort|ev|neut__Number=Sing\n",
    "van          ADP        Prep|voor__AdpType=Prep\n",
    "het          DET        Art|bep|onzijd|neut__Definite=Def|Gender=Neut|PronType=Art\n",
    "klimaat      NOUN       N|soort|ev|neut__Number=Sing\n",
    ",            PUNCT      Punc|komma__PunctType=Comm\n",
    "heeft        VERB       V|hulp|ott|3|ev__Aspect=Imp|Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\n",
    "opnieuw      ADV        Adv|gew|geenfunc|stell|onverv__Degree=Pos\n",
    "een          DET        Art|onbep|zijdofonzijd|neut__Definite=Ind|Number=Sing|PronType=Art\n",
    "recordhoogte NOUN       N|soort|ev|neut__Number=Sing\n",
    "bereikt      VERB       V|trans|verldw|onverv__Subcat=Tran|Tense=Past|VerbForm=Part\n",
    ".            PUNCT      Punc|punt__PunctType=Peri\n",
    "\n",
    "```\n",
    "\n",
    "It is not possible to explain the POS tag in Dutch. Just use tag_ in the third column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠ As of spaCy v3.0, shortcuts like 'nl' are deprecated. Please use the full\n",
      "pipeline package name 'nl_core_news_sm' instead.\n",
      "Collecting nl-core-news-sm==3.4.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/nl_core_news_sm-3.4.0/nl_core_news_sm-3.4.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from nl-core-news-sm==3.4.0) (3.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (1.0.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (3.1.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (1.23.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (21.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (0.10.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (0.6.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (3.0.8)\n",
      "Requirement already satisfied: setuptools in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (63.2.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (1.9.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (8.1.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (2.28.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (3.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (1.0.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (0.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (2.0.7)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (2.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (2022.9.24)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\u0040810\\onedrive - thomas more\\ai project\\2022_2023\\dl_env\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->nl-core-news-sm==3.4.0) (2.1.1)\n",
      "✔ Download and installation successful\n",
      "You can now load the package via spacy.load('nl_core_news_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-20 09:43:07.589404: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-10-20 09:43:07.589727: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-10-20 09:43:10.830106: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'nvcuda.dll'; dlerror: nvcuda.dll not found\n",
      "2022-10-20 09:43:10.830342: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-20 09:43:10.834765: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: G_NB110860\n",
      "2022-10-20 09:43:10.835075: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: G_NB110860\n",
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 22.2.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!py -m spacy download nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "sp = spacy.load(\"nl_core_news_sm\")\n",
    "\n",
    "sen = sp('De concentratie broeikasgassen die bijdragen aan de verandering van het klimaat, heeft opnieuw een recordhoogte bereikt.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De           DET        LID|bep|stan|rest\n",
      "concentratie NOUN       N|soort|ev|basis|zijd|stan\n",
      "broeikasgassen NOUN       N|soort|mv|basis\n",
      "die          PRON       VNW|betr|pron|stan|vol|persoon|getal\n",
      "bijdragen    NOUN       N|soort|mv|basis\n",
      "aan          ADP        VZ|init\n",
      "de           DET        LID|bep|stan|rest\n",
      "verandering  NOUN       N|soort|ev|basis|zijd|stan\n",
      "van          ADP        VZ|init\n",
      "het          DET        LID|bep|stan|evon\n",
      "klimaat      NOUN       N|soort|ev|basis|onz|stan\n",
      ",            PUNCT      LET\n",
      "heeft        AUX        WW|pv|tgw|met-t\n",
      "opnieuw      ADV        BW\n",
      "een          DET        LID|onbep|stan|agr\n",
      "recordhoogte NOUN       N|soort|ev|basis|zijd|stan\n",
      "bereikt      VERB       WW|vd|vrij|zonder\n",
      ".            PUNCT      LET\n"
     ]
    }
   ],
   "source": [
    "for word in sen:\n",
    "    print(f'{word.text:{12}} {word.pos_:{10}} {(word.tag_)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition refers to the identification of words in a sentence as an entity e.g. the name of a person, place, organization, etc. Let's see how the spaCy library performs Named Entity Recognition. Look at the following script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Manchester United, Harry Kane, $90 million)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "sen = sp('Manchester United is looking to sign Harry Kane for $90 million.')\n",
    "\n",
    "print(sen.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that three named entities were identified. To see the detail of each named entity, you can use the text, label, and the spacy.explain method which takes the entity object as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manchester United - ORG - Companies, agencies, institutions, etc.\n",
      "Harry Kane - PERSON - People, including fictional\n",
      "$90 million - MONEY - Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for entity in sen.ents:\n",
    "    print(entity.text + ' - ' + entity.label_ + ' - ' + str(spacy.explain(entity.label_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the POS tags, we can also view named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Manchester United\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking to sign \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Harry Kane\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $90 million\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ". David wants \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    100 Million Dollars\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "sen = sp('Manchester United is looking to sign Harry Kane for $90 million. David wants 100 Million Dollars.')\n",
    "displacy.render(sen, style='ent', jupyter=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DL_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "775b7576bf7a34da706ed620d7f0d2338b0743a1fe22363e0994f105195362b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
