{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR-10\n",
    "\n",
    "In this last example, we will be using the famous CIFAR-10 dataset. CIFAR-10 is a large image dataset containing over 60,000 images representing 10 different classes of objects like cats, planes, and cars. The images are full-color RGB, but unfortunately they are fairly small, only 32 x 32.\n",
    "\n",
    "\n",
    "In the exercise below, we still use the somewhat older version of constructing a convolutional neural network, and a seperate image dataloader. In the previous notebook, we also showed the newer version, and that is runs a lot faster. \n",
    "We encourage you to also try this newer way... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./resources/keras.png\"  style=\"height: 350px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Loading and preprocessing the data\n",
    "\n",
    "The first thing we should do is import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.constraints import maxnorm\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load in the dataset. We can do so simply by specifying which variables we want to load the data into, and then using the load_data() function from cifar10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the data\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing we want to do is normalize the input data. If the values of the input data are in too wide a range it can negatively impact how the network performs. In this case, the input values are the pixels in the image, which have a value between 0 to 255.\n",
    "\n",
    "So in order to normalize the data we can simply divide the image values by 255. To do this we first need to make the data a float type, since they are currently integers. We can do this by using the astype() Numpy command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the inputs from 0-255 to between 0 and 1 by dividing by 255\n",
    "    \n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer Encoding vs. One-Hot Encoding\n",
    "\n",
    "Now let's have a look at the values in y_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6]\n",
      " [9]\n",
      " [9]\n",
      " ...\n",
      " [9]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see every category in our dataset is assigned an integer value. For example, frog is 6, truck is 9, and automobile is 1. https://keras.io/api/datasets/cifar10/\n",
    "\n",
    "\n",
    "This is called *label encoding* or __integer encoding__. Integer values have a natural ordered relationship between each other and machine learning algorithms may try to make benefit of this relationship. In the case of categorical variables where no such ordinal relationship exists, this of course is complete nonsense. This would mean for example that bird (2) is less than cat (3). What should be the meaning of this?\n",
    "\n",
    "In this case, __one-hot encoding__ is the solution. This is where the integer encoded variable is removed and a new binary variable is added for each unique integer value.\n",
    "\n",
    "In our case, there are 10 categories and therefore 10 binary variables are needed. A __one__ value is placed in the binary variable for the corresponding category and __zero__ values for the other categories.\n",
    "\n",
    "So: frog is [0 0 0 0 0 1 0 0 0 0] and truck is [0 0 0 0 0 0 0 0 0 1]. By the way, we used one-hot encoding as well for the cats [0 1] and dogs [1 0] classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Numpy command `to_categorical()` is used to one-hot encode the integer categories. We also need to specify the number of classes that are in the dataset, so we know how many neurons the final layer will contain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "class_num = y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the result below with the original values of `y_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the shape of one image. By now, you know what these values mean. Right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Designing the Model - Exercise\n",
    "\n",
    "We've reached the stage where we design the CNN model. I will describe the properties of the model. It's up to you to code the model using Keras.\n",
    "\n",
    "__a) Model type__\n",
    "- The first thing to do is to define the format we would like to use for the model. Keras has several different formats, but `Sequential` is the most commonly used.\n",
    "\n",
    "__b) First convolution__\n",
    "- The first layer of our model is a `convolutional` layer. The number of filters we want is `32`, the size of the filter we want is `3x3`. Don't forget to specify the `input shape` (when creating the first layer). We will use `relu`, the most common activation function. Because we don't want to change the size of the image, add `padding='same'`.\n",
    "\n",
    "- Now make a dropout layer to prevent overfitting, which functions by randomly eliminating some of the connections between the layers. Drop 20% of the existing connections.\n",
    "\n",
    "- Add a batch normalization layer. Batch normalization is a technique for improving the speed, performance, and stability of Neural Networks. The reasons behind its effectiveness involve a lot of math, which is beyond the scope of this course. But if you are interested, you can find a lot of information on the internet.\n",
    "\n",
    "__c) Second convolution__\n",
    "- Add a `convolutional` layer, once again. Use `64` filters this time. The size is `3x3`, activation is `relu` and `padding='same'`.\n",
    "- Add a MaxPooling layer with size 2x2.\n",
    "- Add a Dropout layer (20%).\n",
    "- Conclude with a batch normalization layer.\n",
    "\n",
    "__d) Third convolution__\n",
    "- Repeat exactly the second convolution.\n",
    "\n",
    "__e) Fourth convolution__\n",
    "- Repeat the second convolution with the following changes: Use 128 filters. Omit the MaxPooling layer. Since the images are so small here already, we won't pool more than twice.\n",
    "\n",
    "__f) Flatten__\n",
    "- After you are done with the convolutional layers, you need to flatten the data. To prevent overfitting, add a Dropout layer once more.\n",
    "\n",
    "__g) First dense layer__\n",
    "- Add a dense layer with 256 neurons. The activation funcion is `relu`.\n",
    "- Add a Dropout layer (20%).\n",
    "- Conclude with a batch normalization layer.\n",
    "\n",
    "__h) Second dense layer__\n",
    "- Repeat the first dense layer, but with 128 neurons this time. Note that the number of neurons in succeeding layers decreases, eventually approaching the same number of neurons as there are classes in the dataset (in this case 10). \n",
    "\n",
    "__i) Final layer__\n",
    "- In the final layer, we pass in the number of classes for the number of neurons. Each neuron represents a class, and the output of this layer will be a 10 neuron vector with each neuron storing some probability that the image in question belongs to the class it represents. Finally, the softmax activation function selects the neuron with the highest probability as its output, voting that the image belongs to that class (sigmoid function is used for two classes, whereas the softmax function is used for the multiclasses)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Compiling the model - Exercise\n",
    "\n",
    "Now that you've designed the model, you just have to compile it. Let's specify the number of epochs we want to train for, as well as the optimizer we want to use.\n",
    "\n",
    "The optimizer is what will tune the weights in your network to approach the point of lowest loss. The Adam algorithm is one of the most commonly used optimizers because it gives great performance on most problems. Use `categorical_crossentropy` as loss function and `accuracy` as metrics.\n",
    "\n",
    "Print out the model summary to see what the whole model looks like. The total number of parameters should be 2,264,458."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Training the model - Exercise\n",
    "\n",
    "Let's train our model now! To do this, all we have to do is call the fit() function on the model and pass in the chosen parameters. We will store the training loss values and metrics in a history object, so we can visualize the training process later.\n",
    "\n",
    "We are going to train the model in 15 epochs, using a batch size of 64. We'll be training on 50,000 samples and validating on 10,000 samples. Since our CNN is rather complex, this will take a very long time. \n",
    "\n",
    "So it might be a better idea to train the model in 15 epochs at home, while doing other things. For now, train the model in 1 epoch. The accuracy will be crap, but you can at least complete the rest of the Notebook. Later at home, you can try to achieve a better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit( ... )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__It might be a good idea to save the weights of your trained model!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('saved_models/modelcifar-10.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Visualizing the training process\n",
    "\n",
    "With this simple function we will be able to plot our training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotLosses(history):  \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotLosses(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Make predictions - Exercise\n",
    "\n",
    "With the model trained, we can use it to make predictions about the test images.\n",
    "\n",
    "Draw the first 25 __test images__ (5 images in a row). Below the image you print the actual value (a) and the predicted value (p). If they are the same the textcolor is green, red otherwise. Tune the subplot layout and create a little bit of space between the subplots. You should get something like this:\n",
    "\n",
    "<img src=\"./resources/uit.png\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('DL_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "775b7576bf7a34da706ed620d7f0d2338b0743a1fe22363e0994f105195362b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
