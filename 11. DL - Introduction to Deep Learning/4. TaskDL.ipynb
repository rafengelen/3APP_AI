{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## General Guidelines - Tasks: \n",
    "### **ipynb - pdf - Streamlit app**\n",
    "\n",
    "During the practical part of this course, each student has to complete three individual tasks, one for each of the three domains of AI that we'll cover.\n",
    "For each of those tasks, a notebook with the task description, and some general steps of what we expect to be covered in the solution, will be provided. \n",
    "We also expect you to **use Generative AI tools** to assist you in creating a solution. So, definetely have a look at our Generative AI Policy description as well.\n",
    "\n",
    "Each of the tasks, deals with a problem that first has to be solved via Python code, in a **Jupyter notebook (.ipynb) format**, so you can switch between code and comments (in Markdown). Also make sure, that you can **export the .ipynb into a pdf**, including the output of the code cells, since this will also be a **requirement on the practical exam**. You can even extend your notebook, to create a full blown report (see below).\n",
    "\n",
    "Next, this code has to be transformed into a **Streamlit application** (a very user friendly Python based web application framework). Streamlit is not part of the course material itself, and you're expected to master the basic principles yourself. It's really straightforward to use, especially if you use Bing AI or other GenAI tools. It will lift your simple tasks from code 'living in a notebook', to a nice visual web application, in a matter of minutes (after some practise). Another advantage of using Streamlit, is that you can even include the app in your online portfolio, if you want to.\n",
    "\n",
    "Finally, each tasks should have a **report in pdf**, where you explain your application, the different steps you took to get to the result, the reasoning/intention behind those steps, extra visualisations to make the solution more explainable, problems/roadblocks/speedbumps you encountered and how you conquered them, etc. And, **including a section on the GenAI tools** you employed (see GenAI Policy), and **the prompts** you used to generate your desired result. This report can be part of the exported Jupyter notebook (preferred), or can be a completely stand-alone new report.\n",
    "\n",
    "\n",
    "### General deliverables for each of the tasks:\n",
    "- a pdf version of your notebook, including the cell outputs (preferrably also including the reporting part)\n",
    "- a streamlit app. Preferrably a 'private' URL where we can test your application live. If your app isn't live, include screenshots in your report as well.\n",
    "- a pdf report, if you didn't include it in the pdf of your notebook\n",
    "\n",
    "\n",
    "### General Grading Guidelines\n",
    "\n",
    "Each task will have their own specific grading, but as a general guideline:\n",
    "\n",
    "The grading will be on a scale of 0 (insufficient completion of task, including not completing one or more subtasks, and/or insufficient reporting style) to 0.5 (completed every element of the task, but poor reporting style), to 1 (task completed, impressive solution, good reporting style)\n",
    "\n",
    "### Streamlit\n",
    "\n",
    "Streamlit is an easy to use Python webframework, and you can find many online tutorials to help you transform your code into a live Streamlit webapp. Remember: you are encouraged to use GenAI tools to get the job done.\n",
    "\n",
    "A couple of YouTube channels, explaining the possibilies of Streamlit very well:\n",
    "\n",
    "- Misra Turp: https://www.youtube.com/watch?v=-IM3531b1XU&list=PLM8lYG2MzHmRpyrk9_j9FW0HiMwD9jSl5&ab_channel=M%C4%B1sraTurp\n",
    "- Data Professor: https://www.youtube.com/watch?v=ZZ4B0QUHuNc&list=PLtqF5YXg7GLmCvTswG32NqQypOuYkPRUE&ab_channel=DataProfessor\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task DL: Image Classification\n",
    "\n",
    "In class, we've covered how to build our own CNN Deep Learning network from scratch, using a couple of 2D convolutions and pooling layers, combined with non-linear activation functions like the ReLu, mixed with some extra regularisation techniques like Dropout, BatchNormalisation, and image augmentation/generation, to arrive at a flatten layer that can act as the input for our dense, fully connected layers to do a binary or categorical classification. \n",
    "\n",
    "We've probably also mentioned that designing such a network, is like an art-form. There are no strict guidelines on which choices are best, how many layers, how many neurons per layer, how...\n",
    "It's easy to design 'a' CNN network, but it's hard to design a good one! It usually comes down to trial and error!\n",
    " \n",
    "And that's what this challenge is all about. \n",
    "\n",
    "Can we design a CNN network from scratch, that performs pretty good on a dataset, that we've collected ourselves? And, if we compare our network's performance to [Google's TeachableMachines](https://teachablemachine.withgoogle.com/) performance on that same dataset, how do we compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your challenge: \n",
    "\n",
    "##### 1. Scrape together your own image dataset \n",
    "\n",
    "- Choose (at least) **5 different categories of images** you would like to make an image classifier for. What are you passionate about? Is it something 'visual', are there a lot of images of, and are there at least 5 different flavors of it? \n",
    "- Next, research & build your own Python scraper program (Scrapy, BeautifulSoup, Selenium,...) to automatically collect your image data from Google Images, Bing, Pexel, specialised sites, etc. Ofcourse you can re-use existing code snippets (even your scraper from the DataScience course), but make sure you make them your own (understand the code, tweak it to better fit your needs, extend it,...), and **reference** them. Add your own comments so we can see that you understand the code. \n",
    "    - **You cannot use pre-build or build-in scraper programs (.exe, browser plugins like DownloadAllImages,...).** \n",
    "    - It has to be a Python script, so you can automate your scraping process, and you can easily adjust it to scrape for more images, if need be. You should be able to collect at least 100 images of each category, but more is always better for deep learning (like +1000 images per category). \n",
    "- Document why you chose those categories of images, where you got your data, did you run into any speedbumps, any coding issues with the scraper,...?  \n",
    "\n",
    "##### 2. EDA & Prep the data\n",
    "\n",
    "- Now that you have your dataset, do a small EDA (how many images from each class, maybe visualize a few).\n",
    "- Next, split your data into a train, validation, and test set (by using ImageDataGenerator - see lesson on CNN). \n",
    "- Document the data loading step. No speedbumps? Smooth ride, or did you encounter some issues?\n",
    "\n",
    "##### 3. Design a CNN network with some regularisation options\n",
    "\n",
    "- Design and compile your own CNN network from scratch. Ofcourse, you can get inspiration from the notebooks from class, or from other designs you can find on the Internet.  \n",
    "- **Note**: at first, **don't use transfer learning (= pretrained network) for this challenge**. We want you to manually design a network, and check its performance. \n",
    "\n",
    "But, if you want top notch performance, and want a fighting chance to beat Google's TeachableMachine, you can extend your assignment, and research how you could use a pretrained 'resnet' in Keras, using the pretrained weights from 'imagenet'. But, **only after you've compared it to your own design**.\n",
    "\n",
    "##### 4. Train your model, but don't overfit (plot the training and validation/test error)\n",
    "\n",
    "- Train your network, but watch your validation error. **Don't overfit!**. \n",
    "- Make sure you plot your training and validation error, and include it in the report.\n",
    "- When you're satisfied with your model, compute the confusion matrix on the test set (see also the ML challenge)\n",
    "\n",
    "**Make sure you also keep the printouts (and plots) of the code cells**\n",
    "\n",
    "##### 5. Compare your model's performance to Google's Teachable Machine, using the same training dataset\n",
    "\n",
    "- Compare/benchmark with Google's TeachableMachine (train with the same images, i.e. train + validation set), hit the advanced button to get the confusion matrix, and take a screenshot of it to include it in your report. We know this is a the confusion matrix made from the training job, and normally we use the confusion matrix on the test set to compare, but this will do for our quick comparison. \n",
    "\n",
    "##### 6. Combine everything into one notebook, and export to pdf\n",
    "\n",
    "- As for all assignments, make sure you can create a nice pdf report of everything you did, with a small intro, and at the end a conclusion. Don't forget to include the GenAI segment!\n",
    "\n",
    "##### 7. Streamlit app\n",
    "- Create a streamlit app from your code. Visualize the EDA (balanced dataset? some sample images), and add some controls to control the training: a slider for the amount of epochs you can train your model, visualize the train and validation error, etc. If you want to go all out, you can even give the user some options to turn on/off some of the regularization options of the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To be submitted \n",
    "\n",
    "\n",
    "- a pdf version of your notebook, including the cell outputs and comments (preferrably **also including the reporting part**)\n",
    "- a pdf report (if you didn't include it in the pdf of your notebook)\n",
    "- a streamlit app \n",
    "    - as an URL where we can test your application live (we don't need the streamlit code in this case) - Checkout the Streamlit Community Cloud to deploy your app via Github\n",
    "    - if your app isn't live, include screenshots in your report as well, and include the code of the app in a seperate section of your pdf.\n",
    "\n",
    "We know that solutions can be found on the internet, or via GenAI tools. You may use those to get started, but don't copy them blindly! You are going to get the most satisfaction out of this assignment if you try to code it - at least partially - yourself. And don't forget a **section in your report about the GenAI usage**.\n",
    "\n",
    "## Grading\n",
    "\n",
    "During the grading we will take into account:\n",
    "\n",
    "- is your code neatly structured?\n",
    "- do you have enough comments / explanations? Do they reflect that you have a solid understanding of the code?\n",
    "- did you do a good job scraping together a usefull dataset?\n",
    "- how good is your (over/well/under) fit on the data, and your comparison to TeachableMachine?\n",
    "- is the report clear, professional, and does it give a good overall impression? No _bric a brac_\n",
    "- how impressive is the live app, or the screenshots when the app isn't live? Does it do what we asked it to do?\n",
    "- how's the usage of GenAI in this assignment?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ac19fea7ca157f65070a947d4f1cdd50788aaffbd4bca54f0297c51647d6a9e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
