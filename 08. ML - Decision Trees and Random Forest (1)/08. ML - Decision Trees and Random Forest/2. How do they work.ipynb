{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How do they work?\n",
    "\n",
    "Decision Trees are a class of very powerful Machine Learning models capable of achieving high accuracy in many tasks while being **highly interpretable**. What makes decision trees special is really their clarity of information representation. The *knowledge* learned by a decision tree through training is directly formulated into a hierarchical structure. This structure holds and displays the knowledge in such a way that it can easily be understood, even by non-experts.\n",
    "\n",
    "Let's start with the example in the following video (from 0:25 till 9:25) (the sound is a little bit bad).\n",
    "\n",
    "<a href=\"https://www.youtube.com/embed/eKD5gxPPeY0?start=25&end=565\"><img src=\"resources/video1.png\" width=\"400\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predict if John will play tennis\n",
    "\n",
    "We've observed John over a number of days and recorded various things that we think might influence John playing tennis or not.\n",
    "\n",
    "<img src=\"./resources/ten1.png\" style=\"height: 400px\"/>\n",
    "\n",
    "We will try to predict if John will play tennis on day 15, a rainy day with high humidity and weak wind. \n",
    "\n",
    "**BTW:** we'll only use the features 'outlook', 'humidity', and 'wind' to predict if he will play. We won't be taking the daily trend data into account (the sequence of days), for our model, i.e. we could look at the data as a time-series data, and predict if he will play based on his 'weekly or daily-routine'. But, trend analysis is out of scope for this course. If interested, have a look at ARIMA models, and Facebook's Prophet model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm\n",
    "\n",
    "- Look at the attributes (outlook, humidity, wind) and split the data using those attributes into smaller subsets.\n",
    "- Check if all subsets are pure (a subset is pure if it contains only yes’s and no’s).\n",
    "    * if the answer is yes: stop\n",
    "    * if the answer is no: repeat with the non pure datasets\n",
    "    * note: this algorithm is recursive by nature (keep splitting until you hit a base class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Intuitive solution\n",
    "\n",
    "Let’s say we take the Outlook attribute to split the dataset. The Outlook has 3 possible values i.e. Sunny, Overcast and Rain. So we split our dataset into 3 smaller subsets as shown below:\n",
    "\n",
    "<img src=\"./resources/ten2.png\" style=\"height: 300px\"/>\n",
    "\n",
    "After splitting on the Outlook attribute we have one pure node in which John always plays. So intuitively we can say that when Outlook is Overcast John always plays.\n",
    "\n",
    "On the other hand, we have 2 impure sets i.e. in those subsets we have no idea if John will play or not? For those subsets, further splitting is required. Suppose for Sunny as an Outlook subset, we take Humidity to split further, then the tree looks as follows: \n",
    "\n",
    "<img src=\"./resources/ten3.png\" style=\"height: 300px\"/>\n",
    "\n",
    "Now we have 2 pure and 1 impure subset. With Sunny Outlook and High Humidity John does not play and with Normal Humidity, John plays.\n",
    "\n",
    "For the Rainy subset, because it is not pure, we split further on Wind.\n",
    "\n",
    "<img src=\"./resources/ten4.png\" style=\"height: 400px\"/>\n",
    "\n",
    "At this point, we have partitioned our entire training set into smaller subsets based on a small set of rules and each subset is pure i.e. in all subsets John plays or not. The final decision tree after replacing instances by labels will look like the following:\n",
    "\n",
    "<img src=\"./resources/ten5.png\" style=\"height: 300px\"/>\n",
    "\n",
    "Will John play tennis on day 15, a rainy day with high humidity and weak wind?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer?\n",
    "# Yes John will play tennis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How does a decision tree work?\n",
    "\n",
    "The most difficult problem building a decision tree is selecting  the best attribute to split the dataset into subsets. Following video will explain one possible way to do this (from 9:12 till 14:30).\n",
    "\n",
    "<a href=\"https://www.youtube.com/embed/RmajweUFKvM?start=552&end=870\"><img src=\"resources/video2.png\" width=\"400\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Questions\n",
    "\n",
    "Explain the following terms: entropy, information gain, leaf node, decision node, root node.\n",
    "\n",
    "When you split the data, do you want the information gain to be the lowest/highest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy\n",
    "# measure of randomness or unpredictability in the dataset\n",
    "\n",
    "# information gain\n",
    "# measure of decrease in entropy after dataset is split\n",
    "# entropy1 - entropy2 = information gain\n",
    "# hoe beter e2 gekozen hoe meer gain\n",
    "\n",
    "# leaf node\n",
    "# einde aan de bodem, bevat de classificatie\n",
    "\n",
    "# decision node\n",
    "# punt waar er een beslissing moet genomen worden\n",
    "\n",
    "# root node\n",
    "# bovenste decision node\n",
    "\n",
    "# lowest / highest\n",
    "# you want the information gain to be the highest. So create decisions that have less entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Exercises\n",
    "\n",
    "Do the following datasets have a high, low or zero entropy?\n",
    "\n",
    "<img src=\"./resources/entropy.png\"/>\n",
    "\n",
    "Can you compute the entropy for the datasets below (the first is already done)? You can use this Log Base 2 calculator (https://www.miniwebtool.com/log-base-2-calculator/). Since Log Base 2 is negative for values between 0 and 1, you have to take the absolute value.\n",
    "\n",
    "Does the entropy really decrease when the dataset is more pure?\n",
    "\n",
    "<img src=\"./resources/calculate.png\" style=\"height: 500px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) high \n",
    "# b)  zero\n",
    "\n",
    "# entropy dataset 1: formula: (1/5)log2(1/5) + (4/5)log2(4/5) = 0.2 x 2.32 + 0.8 x 0.32 = 0.46 + 0.25 = 0.71\n",
    "# entropy dataset 2: formula: (1/3)log2(1/3) + (2/3)log2(2/3) = 0.92\n",
    "# entropy dataset 3: formula: 0\n",
    "\n",
    "# Does the entropy really decreases when the dataset is more pure?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark for completeness: There's another method to measure the impurity of a dataset, namely the Gini index, but it will work in much the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final remarks on Decision Trees\n",
    "\n",
    "## Pros\n",
    "\n",
    "- **Easy to understand and interpret**. At each node, we are able to see exactly what decision our model is making.\n",
    "\n",
    "- Require very little data preparation. Decision trees work quite well out of the box without major data-preparation. For instance, we didn't have to normalize our data, something that is a must for other algorithms. Normalizing data: making sure that some features don't have range between 0-10 for instance, and others a range of -1M to +1M: because then the big range might 'overpower' the smaller range, and bias your model towards the big range.\n",
    "\n",
    "## Cons\n",
    "\n",
    "- **Overfitting** is quite common with decision trees simply due to the nature of their training. Decision trees also tend to overfit on their training data, making them perform poorly if data shown to them later don’t closely match to what they were trained on. In other words, decision trees don’t generalize well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('ML_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3762fa38335283772957b8ad4850e1d344741e862c3aa7ec1c26d17780db6975"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
