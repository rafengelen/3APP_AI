{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does it work?\n",
    "\n",
    "Decision trees work great with the data used to create them, but they are not flexible when it comes to classify new samples. Random Forests combine the simplicity of decision trees with flexibility resulting in a vast improvement in accuracy.\n",
    "\n",
    "Watch the following video (from 0:25 till 9:27) to see how we can improve the accuracy of our predictions using a random forest.\n",
    "\n",
    "<a href=\"https://www.youtube.com/embed/J4Wdy0Wc_xQ?start=24&end=567\"><img src=\"resources/video3.png\" width=\"400\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Algorithm\n",
    "\n",
    "We can understand the working of the Random Forest algorithm with the help of the following steps:\n",
    "\n",
    "1. Select random samples from a given dataset (= **bootstrapped datasets**).\n",
    "2. Construct a decision tree for each sample (using only a random subset of variables =**feature bootstrapping**) and get a prediction result from each decision tree.\n",
    "3. Perform a vote for each predicted result.\n",
    "4. Select the prediction result with the most votes as the final prediction.\n",
    "\n",
    "The following diagram illustrates how it works:\n",
    "\n",
    "<img src=\"./resources/voting.png\" style=\"height: 400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pros\n",
    "\n",
    "Using a bootrstrapped sample and considering only a subset of the variables at each step results in a wide variety of trees. The variety is what makes random forests more effective than individual decision trees.\n",
    "\n",
    "<img src=\"./resources/forest.png\" style=\"height: 300px\"/>\n",
    "\n",
    "- Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.\n",
    "- It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Questions\n",
    "\n",
    "Can you explain following terms: out-of-bag-dataset, out-of-bag-error? How can we estimate the accuracy of a random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out-of-bag-dataset:\n",
    "# out-of-bag-error: \n",
    "# How can we estimate the accuracy of a random forest?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bagging\n",
    "\n",
    "Bagging is a colloquial term for bootstrap aggregation:\n",
    "\n",
    "- Bootstrapping: select random samples from a given dataset\n",
    "- Feature bagging: use only a random subset of variables to build each decision tree\n",
    "- Aggregation: after all the models have been built, their outputs must be aggregated into a single coherent prediction for the larger model.\n",
    "\n",
    "As you can see, the bootstrapping and feature bagging process produces wildly different decision trees than just the single decision tree applied to all the data (here for the iris dataset).\n",
    "\n",
    "<img src=\"./resources/iris_sampling.gif\" style=\"height: 300px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
